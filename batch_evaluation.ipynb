{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ§ª Intelligent RAG Chatbot - Batch Evaluation\n",
        "\n",
        "This notebook performs comprehensive batch evaluation of the Intelligent Multi-Stage RAG Chatbot using TruLens and Snowflake AI Observability.\n",
        "\n",
        "**Purpose**: Run systematic evaluations with RAG triad metrics on legal document analysis tasks.\n",
        "\n",
        "**Prerequisites**:\n",
        "- Install packages: `trulens-core==1.5.2`, `trulens-providers-cortex==1.5.2`, `trulens-connectors-snowflake==1.5.2`\n",
        "- Ensure RAG objects exist (CS_DOCUMENTS_METADATA, CS_DOCUMENTS_CHUNKS)\n",
        "- Required privileges: SNOWFLAKE.CORTEX_USER, AI_OBSERVABILITY_EVENTS_LOOKUP\n",
        "\n",
        "**Why Notebook?**: TruLens batch evaluation works better in notebook environment due to execution context requirements.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ› ï¸ Setup Environment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "from typing import Dict, List, Any\n",
        "from snowflake.snowpark.context import get_active_session\n",
        "from snowflake.core import Root\n",
        "from snowflake.cortex import Complete\n",
        "\n",
        "# Enable TruLens OpenTelemetry tracing\n",
        "os.environ[\"TRULENS_OTEL_TRACING\"] = \"1\"\n",
        "\n",
        "# TruLens imports\n",
        "from trulens.core.otel.instrument import instrument\n",
        "from trulens.otel.semconv.trace import SpanAttributes\n",
        "from trulens.apps.app import TruApp\n",
        "from trulens.connectors.snowflake import SnowflakeConnector\n",
        "from trulens.core.run import Run, RunConfig\n",
        "\n",
        "# Get active session\n",
        "session = get_active_session()\n",
        "print(f\"âœ… Connected to Snowflake: {session.get_current_database()}.{session.get_current_schema()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“Š Setup Observability Database & Schema\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "CREATE DATABASE IF NOT EXISTS AI_OBSERVABILITY_DB;\n",
        "CREATE SCHEMA IF NOT EXISTS AI_OBSERVABILITY_DB.EVALUATION_SCHEMA;\n",
        "USE SCHEMA AI_OBSERVABILITY_DB.EVALUATION_SCHEMA;\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify observability context\n",
        "current_db = session.get_current_database()\n",
        "current_schema = session.get_current_schema()\n",
        "print(f\"ðŸ“Š Observability Context: {current_db}.{current_schema}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ¤– Create Instrumented RAG Class\n",
        "\n",
        "Recreate the RAG logic with proper TruLens instrumentation for notebook execution.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NotebookRAGChatbot:\n",
        "    \"\"\"\n",
        "    Simplified RAG chatbot for notebook-based batch evaluation.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, session):\n",
        "        self.session = session\n",
        "        self.root = Root(session)\n",
        "        \n",
        "        # Store original context to restore later\n",
        "        self.original_db = session.get_current_database()\n",
        "        self.original_schema = session.get_current_schema()\n",
        "        \n",
        "        print(f\"ðŸ” RAG will use context: {self.original_db}.{self.original_schema}\")\n",
        "    \n",
        "    @instrument(\n",
        "        span_type=SpanAttributes.SpanType.RETRIEVAL,\n",
        "        attributes={\n",
        "            SpanAttributes.RETRIEVAL.QUERY_TEXT: \"query\",\n",
        "            SpanAttributes.RETRIEVAL.RETRIEVED_CONTEXTS: \"return\"\n",
        "        }\n",
        "    )\n",
        "    def query_metadata_search_service(self, query: str, limit: int = 10) -> List[Dict]:\n",
        "        \"\"\"Query metadata search service.\"\"\"\n",
        "        try:\n",
        "            # Temporarily switch to RAG context\n",
        "            self.session.sql(f\"USE SCHEMA {self.original_db}.{self.original_schema}\").collect()\n",
        "            \n",
        "            search_service = (\n",
        "                self.root.databases[self.original_db]\n",
        "                .schemas[self.original_schema]\n",
        "                .cortex_search_services[\"CS_DOCUMENTS_METADATA\"]\n",
        "            )\n",
        "            \n",
        "            response = search_service.search(\n",
        "                query=query,\n",
        "                columns=[\"FILENAME\", \"FILE_URL\", \"CONTENT_METADATA\"],\n",
        "                limit=limit\n",
        "            )\n",
        "            \n",
        "            # Switch back to observability context\n",
        "            self.session.sql(\"USE SCHEMA AI_OBSERVABILITY_DB.EVALUATION_SCHEMA\").collect()\n",
        "            \n",
        "            return response.results if response.results else []\n",
        "        except Exception as e:\n",
        "            print(f\"Error in metadata search: {e}\")\n",
        "            return []\n",
        "    \n",
        "    @instrument(\n",
        "        span_type=SpanAttributes.SpanType.RETRIEVAL,\n",
        "        attributes={\n",
        "            SpanAttributes.RETRIEVAL.QUERY_TEXT: \"query\",\n",
        "            SpanAttributes.RETRIEVAL.RETRIEVED_CONTEXTS: \"return\"\n",
        "        }\n",
        "    )\n",
        "    def query_chunks_search_service(self, query: str, relevant_filenames: List[str], limit: int = 5) -> List[Dict]:\n",
        "        \"\"\"Query chunks search service with filtering.\"\"\"\n",
        "        try:\n",
        "            # Temporarily switch to RAG context\n",
        "            self.session.sql(f\"USE SCHEMA {self.original_db}.{self.original_schema}\").collect()\n",
        "            \n",
        "            search_service = (\n",
        "                self.root.databases[self.original_db]\n",
        "                .schemas[self.original_schema]\n",
        "                .cortex_search_services[\"CS_DOCUMENTS_CHUNKS\"]\n",
        "            )\n",
        "            \n",
        "            if len(relevant_filenames) == 1:\n",
        "                filter_dict = {\"@eq\": {\"FILENAME\": relevant_filenames[0]}}\n",
        "            else:\n",
        "                filter_dict = {\"@or\": [{\"@eq\": {\"FILENAME\": filename}} for filename in relevant_filenames]}\n",
        "            \n",
        "            response = search_service.search(\n",
        "                query=query,\n",
        "                columns=[\"FILENAME\", \"CHUNK_TEXT\", \"FILE_URL\"],\n",
        "                filter=filter_dict,\n",
        "                limit=limit\n",
        "            )\n",
        "            \n",
        "            # Switch back to observability context\n",
        "            self.session.sql(\"USE SCHEMA AI_OBSERVABILITY_DB.EVALUATION_SCHEMA\").collect()\n",
        "            \n",
        "            return response.results if response.results else []\n",
        "        except Exception as e:\n",
        "            print(f\"Error in chunks search: {e}\")\n",
        "            return []\n",
        "    \n",
        "    @instrument(span_type=SpanAttributes.SpanType.GENERATION)\n",
        "    def generate_completion(self, user_question: str, context_str: str) -> str:\n",
        "        \"\"\"Generate completion using Cortex LLM.\"\"\"\n",
        "        prompt = f\"\"\"\n",
        "        You are an expert legal document analysis assistant. Based on the provided context from legal contracts,\n",
        "        provide a comprehensive and accurate answer to the user's question.\n",
        "        \n",
        "        Context: {context_str}\n",
        "        \n",
        "        Question: {user_question}\n",
        "        \n",
        "        Answer:\n",
        "        \"\"\"\n",
        "        \n",
        "        return Complete(\"mistral-large2\", prompt)\n",
        "    \n",
        "    @instrument(\n",
        "        span_type=SpanAttributes.SpanType.RECORD_ROOT,\n",
        "        attributes={\n",
        "            SpanAttributes.RECORD_ROOT.INPUT: \"user_question\",\n",
        "            SpanAttributes.RECORD_ROOT.OUTPUT: \"return\"\n",
        "        }\n",
        "    )\n",
        "    def query(self, user_question: str) -> str:\n",
        "        \"\"\"Main query method for batch evaluation.\"\"\"\n",
        "        # Step 1: Get relevant documents from metadata search\n",
        "        metadata_results = self.query_metadata_search_service(user_question)\n",
        "        \n",
        "        if not metadata_results:\n",
        "            return \"I couldn't find relevant documents for your query.\"\n",
        "        \n",
        "        # Step 2: Extract filenames\n",
        "        relevant_filenames = [result[\"FILENAME\"] for result in metadata_results[:3]]  # Top 3\n",
        "        \n",
        "        # Step 3: Get chunks from relevant documents\n",
        "        chunk_results = self.query_chunks_search_service(user_question, relevant_filenames)\n",
        "        \n",
        "        if not chunk_results:\n",
        "            return \"I found relevant documents but couldn't extract specific information.\"\n",
        "        \n",
        "        # Step 4: Build context and generate response\n",
        "        context_chunks = [result[\"CHUNK_TEXT\"] for result in chunk_results]\n",
        "        context_str = \"\\n\\n\".join(context_chunks)\n",
        "        \n",
        "        response = self.generate_completion(user_question, context_str)\n",
        "        \n",
        "        return response\n",
        "\n",
        "# Create RAG instance\n",
        "rag_chatbot = NotebookRAGChatbot(session)\n",
        "print(\"âœ… RAG Chatbot created successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ§ª Test RAG Functionality\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the RAG with a sample query\n",
        "test_response = rag_chatbot.query(\"What are the termination clauses in the distributor agreement?\")\n",
        "print(\"ðŸŽ¯ Test Response:\")\n",
        "print(test_response[:500] + \"...\" if len(test_response) > 500 else test_response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“ Register App with TruLens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Register the app with TruLens\n",
        "tru_connector = SnowflakeConnector(snowpark_session=session)\n",
        "\n",
        "app_name = \"intelligent_rag_chatbot\"\n",
        "app_version = \"notebook_evaluation\"\n",
        "\n",
        "tru_app = TruApp(\n",
        "    rag_chatbot,\n",
        "    app_name=app_name,\n",
        "    app_version=app_version,\n",
        "    connector=tru_connector\n",
        ")\n",
        "\n",
        "print(f\"âœ… TruApp registered: {app_name} v{app_version}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“Š Create Evaluation Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "CREATE TABLE IF NOT EXISTS LEGAL_RAG_EVALUATION_DATASET (\n",
        "    query STRING,\n",
        "    ground_truth_response STRING,\n",
        "    category STRING,\n",
        "    difficulty STRING\n",
        ");\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample evaluation queries for legal document analysis\n",
        "evaluation_data = [\n",
        "    {\n",
        "        \"query\": \"What are the termination clauses in the NETGEAR distributor agreement?\",\n",
        "        \"ground_truth_response\": \"The NETGEAR distributor agreement contains specific termination provisions that allow for termination with notice under certain conditions.\",\n",
        "        \"category\": \"termination\",\n",
        "        \"difficulty\": \"medium\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Compare the IP ownership provisions between the development and endorsement agreements\",\n",
        "        \"ground_truth_response\": \"The development agreement typically assigns IP rights to the developer, while the endorsement agreement maintains IP with the original owner.\",\n",
        "        \"category\": \"comparison\",\n",
        "        \"difficulty\": \"hard\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"What are the liability limitations in the hosting agreement?\",\n",
        "        \"ground_truth_response\": \"The hosting agreement includes liability caps and exclusions for certain types of damages.\",\n",
        "        \"category\": \"liability\",\n",
        "        \"difficulty\": \"medium\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"What governing law provisions are common across all agreements?\",\n",
        "        \"ground_truth_response\": \"Most agreements specify the jurisdiction and governing law for dispute resolution.\",\n",
        "        \"category\": \"general\",\n",
        "        \"difficulty\": \"easy\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"How do renewal terms vary across different contract types?\",\n",
        "        \"ground_truth_response\": \"Renewal terms differ based on contract type, with some automatic renewals and others requiring explicit agreement.\",\n",
        "        \"category\": \"renewal\",\n",
        "        \"difficulty\": \"medium\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Check if data exists\n",
        "count_result = session.sql(\"SELECT COUNT(*) FROM LEGAL_RAG_EVALUATION_DATASET\").collect()\n",
        "current_count = count_result[0][0]\n",
        "\n",
        "if current_count == 0:\n",
        "    # Insert evaluation data\n",
        "    for item in evaluation_data:\n",
        "        session.sql(f\"\"\"\n",
        "        INSERT INTO LEGAL_RAG_EVALUATION_DATASET \n",
        "        (query, ground_truth_response, category, difficulty)\n",
        "        VALUES ('{item[\"query\"]}', '{item[\"ground_truth_response\"]}', '{item[\"category\"]}', '{item[\"difficulty\"]}')\n",
        "        \"\"\").collect()\n",
        "    \n",
        "    print(f\"âœ… Inserted {len(evaluation_data)} evaluation queries\")\n",
        "else:\n",
        "    print(f\"ðŸ“Š Found {current_count} existing evaluation queries\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "SELECT * FROM LEGAL_RAG_EVALUATION_DATASET;\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸš€ Create and Execute Evaluation Run\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create evaluation run configuration\n",
        "run_name = \"legal_rag_batch_eval_v1\"\n",
        "\n",
        "run_config = RunConfig(\n",
        "    run_name=run_name,\n",
        "    dataset_name=\"LEGAL_RAG_EVALUATION_DATASET\",\n",
        "    description=\"Batch evaluation of intelligent multi-stage RAG on legal documents\",\n",
        "    label=\"legal_rag_notebook_eval\",\n",
        "    source_type=\"TABLE\",\n",
        "    dataset_spec={\n",
        "        \"input\": \"query\",\n",
        "        \"ground_truth_output\": \"ground_truth_response\",\n",
        "    },\n",
        "    llm_judge_name=\"mistral-large2\"\n",
        ")\n",
        "\n",
        "# Add run to TruLens\n",
        "run: Run = tru_app.add_run(run_config=run_config)\n",
        "print(f\"âœ… Created evaluation run: {run_name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scale up warehouse for evaluation\n",
        "import pandas as pd\n",
        "\n",
        "current_wh = session.get_current_warehouse().strip('\"')\n",
        "print(f\"ðŸ­ Current warehouse: {current_wh}\")\n",
        "\n",
        "# Scale up to Large for evaluation\n",
        "session.sql(f\"ALTER WAREHOUSE {current_wh} SET WAREHOUSE_SIZE='LARGE'\").collect()\n",
        "print(f\"ðŸ“ˆ Scaled up warehouse: {current_wh} â†’ LARGE\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start the evaluation run\n",
        "print(\"ðŸš€ Starting batch evaluation...\")\n",
        "print(\"This will execute the RAG on all test queries and collect traces.\")\n",
        "\n",
        "try:\n",
        "    run.start()\n",
        "    print(\"âœ… Batch evaluation completed successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Evaluation failed: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute RAG triad metrics\n",
        "print(\"ðŸ“Š Computing evaluation metrics...\")\n",
        "\n",
        "try:\n",
        "    run.compute_metrics([\n",
        "        \"answer_relevance\",\n",
        "        \"context_relevance\",\n",
        "        \"groundedness\",\n",
        "    ])\n",
        "    print(\"âœ… Metrics computed successfully!\")\n",
        "    print(\"ðŸ“ˆ RAG Triad metrics: Answer Relevance, Context Relevance, Groundedness\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Metrics computation failed: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scale warehouse back down\n",
        "session.sql(f\"ALTER WAREHOUSE {current_wh} SET WAREHOUSE_SIZE='X-SMALL'\").collect()\n",
        "print(f\"ðŸ“‰ Scaled down warehouse: {current_wh} â†’ X-SMALL\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“Š View Evaluation Results\n",
        "\n",
        "To view the detailed evaluation results:\n",
        "\n",
        "1. **Navigate to Snowsight** â†’ **AI & ML** â†’ **Evaluations**\n",
        "2. **Select Application**: `intelligent_rag_chatbot`\n",
        "3. **Select Run**: `legal_rag_batch_eval_v1`\n",
        "4. **View Metrics**: RAG triad scores and detailed traces\n",
        "5. **Analyze Results**: Individual query performance and overall statistics\n",
        "\n",
        "### ðŸŽ¯ What You'll See:\n",
        "\n",
        "- **Answer Relevance**: How well responses address the user's question\n",
        "- **Context Relevance**: Quality of retrieved documents/chunks\n",
        "- **Groundedness**: How well responses are supported by retrieved context\n",
        "- **Detailed Traces**: Step-by-step execution with timing and intermediate results\n",
        "- **Comparative Analysis**: Performance across different query types and difficulties\n",
        "\n",
        "### ðŸ“ˆ Success Criteria:\n",
        "\n",
        "- **Answer Relevance** > 0.8: Responses directly address queries\n",
        "- **Context Relevance** > 0.7: Retrieved documents are pertinent\n",
        "- **Groundedness** > 0.8: Responses are well-supported by evidence\n",
        "\n",
        "### ðŸ”„ Hybrid Evaluation Approach:\n",
        "\n",
        "- **Notebook**: Systematic batch evaluation with TruLens RAG triad metrics\n",
        "- **Streamlit App**: Real-time user feedback and interactive experience\n",
        "- **Combined Insights**: Professional evaluation + user satisfaction data\n",
        "\n",
        "ðŸŽ‰ **Congratulations!** You've successfully run a comprehensive batch evaluation of your Intelligent Multi-Stage RAG Chatbot!\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
